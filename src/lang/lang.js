// This file was generated by lezer-generator. You probably shouldn't edit it.
import {LRParser} from "@lezer/lr"
import {keyword} from "./tokenizer.js"
export const parser = LRParser.deserialize({
  version: 14,
  states: "#YOYQROOO!{QRO'#CmOOQQ'#Ct'#CtOOQQ'#Co'#CoOYQROOO#SQPO'#CnQOQPOOOOQQ,59X,59XO#XQRO,59XOOQQ-E6m-E6mO#`QRO,59YOOQQ1G.s1G.sOOQQ'#Cp'#CpO#hQRO1G.tOOQO1G.t1G.tOOQQ-E6n-E6nOOQO7+$`7+$`",
  stateData: "#p~OgOSZOS~OPQOQQORQOSQOUQOVQOWQOXQOYQO]QO^QO`POiTO~OPQOQQORQOSQOUQOVQOWQOXQOYQO]QO^QO`PO~O_VO~P!TO`YO~O_ZO~P!TOT[O_^O~OT[O_`O~O",
  goto: "![iPPPPPPPPPPPPPPPPPjpt!OPPP!UXQOPSWTUOSQSOQWPTXSWQ]YR_]XROPSW",
  nodeNames: "âš  Identifier Directive Command Reporter Extension TurtleVar PatchVar LinkVar Constant Unsupported LineComment Program Numeric String ] [ Application Extensions",
  maxTerm: 25,
  nodeProps: [
    ["openedBy", 15,"["],
    ["closedBy", 16,"]"]
  ],
  skippedNodes: [0,11],
  repeatNodeCount: 2,
  tokenData: "$x~RYXYqYZq]^qpqqrs!S!Q![!q!]!^#U!}#O#a#P#Q#f#X#Y#k~vSg~XYqYZq]^qpqq~!VTOr!Srs!fs#O!S#O#P!k#P~!S~!kO^~~!nPO~!S~!vQ]~!O!P!|!Q![!q~#RP]~!Q![!|~#ZQZ~OY#UZ~#U~#fO`~~#kO_~~#nP#l#m#q~#tP#h#i#w~#zP#X#Y#}~$QP#b#c$T~$WP#g#h$Z~$^P#]#^$a~$dP#c#d$g~$jP#b#c$m~$pP#g#h$s~$xOi~",
  tokenizers: [0, keyword],
  topRules: {"Program":[0,12]},
  tokenPrec: 0
})
